{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c2148f-c1b0-46e0-87f6-2db29e13d5b8",
   "metadata": {},
   "source": [
    "## Masked Autoencoders: Visualization Demo\n",
    "\n",
    "This is a visualization demo using our pre-trained MAE models. No GPU is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eae7403-f458-4f55-a557-4e045bd6f679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\transformers-pytorch\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import torchaudio\n",
    "from torchaudio.compliance import kaldi\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "sys.path.append(\"../\")\n",
    "import importlib\n",
    "import models_mae\n",
    "import librosa\n",
    "import librosa.display\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7797ef-412a-439f-911e-3be294047629",
   "metadata": {},
   "source": [
    "### Define utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3eea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "MELBINS=128\n",
    "TARGET_LEN=1024\n",
    "def prepare_model(chkpt_dir, arch='mae_vit_base_patch16'):\n",
    "    # build model\n",
    "    model = getattr(models_mae, arch)(in_chans=1, audio_exp=True,img_size=(1024,128),decoder_mode=0)\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "def prepare_model1(chkpt_dir, arch='mae_vit_base_patch16'):\n",
    "    # build model\n",
    "    model = getattr(models_mae, arch)(in_chans=1, audio_exp=True,img_size=(1024,128),decoder_mode=1,decoder_depth=16)\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    print(msg)\n",
    "    return model\n",
    "def wav2fbank(filename):\n",
    "\n",
    "    waveform, sr = torchaudio.load(filename)\n",
    "    waveform = waveform - waveform.mean()\n",
    "\n",
    "    # 498 128\n",
    "    fbank = kaldi.fbank(waveform, htk_compat=True, sample_frequency=sr, use_energy=False, \n",
    "                        window_type='hanning', num_mel_bins=MELBINS, dither=0.0, frame_shift=10)\n",
    "    # AudioSet: 1024 (16K sr)\n",
    "    n_frames = fbank.shape[0]\n",
    "    p = TARGET_LEN - n_frames\n",
    "    # cut and pad\n",
    "    if p > 0:\n",
    "        m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "        fbank = m(fbank)\n",
    "    elif p < 0:\n",
    "        fbank = fbank[0:TARGET_LEN, :]\n",
    "    return fbank\n",
    "def norm_fbank(fbank):\n",
    "    norm_mean= -4.2677393\n",
    "    norm_std= 4.5689974\n",
    "    fbank = (fbank - norm_mean) / (norm_std * 2)\n",
    "    return fbank\n",
    "def display_fbank(bank, minmin=None, maxmax=None):\n",
    "    #print(bank.shape, bank.min(), bank.max())\n",
    "    #plt.figure(figsize=(18, 6))\n",
    "    #plt.figure(figsize=(20, 4))\n",
    "    plt.imshow(20*bank.T.numpy(), origin='lower', interpolation='nearest', vmax=maxmax, vmin=minmin,  aspect='auto')\n",
    "    #plt.colorbar()\n",
    "    #S_db = librosa.amplitude_to_db(np.abs(bank.T.numpy()),ref=np.max)\n",
    "    #S_db = bank.T.numpy()\n",
    "    #plt.figure()\n",
    "    #librosa.display.specshow(10*bank.T.numpy())\n",
    "    #plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4b0a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(models_mae)\n",
    "\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c39f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "PatchEmbed_org(\n",
      "  (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\transformers-pytorch\\venv\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(models_mae)\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/53417041/checkpoint-80.pth'\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/55951690/checkpoint-20.pth' #(TF-mask AMAE, 0.7. 0.3)\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/55986074/checkpoint-20.pth' #(TF-mask AMAE) (0.5, 0.2)\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/55986075/checkpoint-20.pth' #(TF-mask AMAE) (0.2, 0.1)\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/54463265/checkpoint-28.pth' # random AMAE\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/55986072/checkpoint-32.pth' # random AMAE (new)\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/56067384/checkpoint-28.pth' # random AMAE (new)\n",
    "#chkpt_dir = '/checkpoint/berniehuang/experiments/56373517/checkpoint-24.pth' # random AMAE, decoder=4, norm_pxl=False\n",
    "chkpt_dir = r'D:\\AudioMAE\\pretrained.pth' # random AMAES, decoder=4, norm_pxl=False\n",
    "model = prepare_model1(chkpt_dir, 'mae_vit_base_patch16')\n",
    "#model = prepare_model1(chkpt_dir, 'mae_vit_base_patch16')\n",
    "#model = prepare_model(chkpt_dir, 'amvmae_vit_base_patch16')\n",
    "print('Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c1fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def run_one_audio(wav_file, model):\n",
    "    fbank = wav2fbank(wav_file)\n",
    "    fbank = norm_fbank(fbank)\n",
    "    x = torch.tensor(fbank)\n",
    "    x = x.unsqueeze(0)\n",
    "    x = x.unsqueeze(0)\n",
    "    mask_ratio = 0.3\n",
    "    _, y, mask, _ = model(x.float(), mask_ratio=mask_ratio)\n",
    "    y_unpatch = model.unpatchify(y)\n",
    "    y_unpatch = torch.einsum('nchw->nhwc', y_unpatch).detach().cpu()\n",
    "    # visualize the mask\n",
    "    mask = mask.detach()\n",
    "    #mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *1)\n",
    "    mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "    #print(x.shape, mask.shape)\n",
    "    # masked image\n",
    "    x = torch.einsum('nchw->nhwc', x)\n",
    "    im_masked = x * (1 - mask)\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "    \n",
    "    minmin=-5\n",
    "    maxmax=10\n",
    "    start=150\n",
    "    end=800\n",
    "    \n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 12]\n",
    "    plt.subplot(3, 1, 1)\n",
    "    display_fbank(x[0][start:end].squeeze(), minmin=minmin, maxmax=maxmax)\n",
    "    #plt.show()\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    display_fbank(im_masked[0][start:end].squeeze(),minmin=minmin, maxmax=maxmax)\n",
    "    #plt.show()\n",
    "    \n",
    "    plt.subplot(3, 1, 3)\n",
    "    display_fbank((y_unpatch[0][start:end]).squeeze(), minmin=minmin, maxmax=maxmax)\n",
    "    #plt.show()\n",
    "    \n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(mask.shape)\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "    display_fbank(im_paste[0][start:end].squeeze(),minmin=minmin, maxmax=maxmax)\n",
    "    \n",
    "    if model.mask_2d:\n",
    "        fn=os.path.basename(wav_file).replace('.wav',f'_2d_{model.mask_t_prob}_{model.mask_f_prob}.pdf')\n",
    "    else:\n",
    "        fn=os.path.basename(wav_file).replace('.wav',f'_{mask_ratio}.pdf')\n",
    "    fn=os.path.join('/checkpoint/berniehuang/mae/vis',fn)\n",
    "    plt.savefig(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "wav_file=r\"D:\\AudioMAE\\Beijing Police car siren-[AudioTrimmer.com].wav\"\n",
    "fbank = wav2fbank(wav_file)\n",
    "fbank = norm_fbank(fbank)\n",
    "x = torch.tensor(fbank)\n",
    "x = x.unsqueeze(0)\n",
    "x = x.unsqueeze(0)\n",
    "mask_ratio = 0.3\n",
    "_, y, mask, _ = model(x.float(), mask_ratio=mask_ratio)\n",
    "y_unpatch = model.unpatchify(y)\n",
    "y_unpatch = torch.einsum('nchw->nhwc', y_unpatch).detach().cpu()\n",
    "# visualize the mask\n",
    "mask = mask.detach()\n",
    "#mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *3)  # (N, H*W, p*p*3)\n",
    "mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *1)\n",
    "mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "#print(x.shape, mask.shape)\n",
    "# masked image\n",
    "x = torch.einsum('nchw->nhwc', x)\n",
    "im_masked = x * (1 - mask)\n",
    "\n",
    "minmin=-5\n",
    "maxmax=10\n",
    "start=150\n",
    "end=800\n",
    "\n",
    "# make the plt figure larger\n",
    "plt.rcParams['figure.figsize'] = [24, 12]\n",
    "plt.subplot(3, 1, 1)\n",
    "display_fbank(x[0][start:end].squeeze(), minmin=minmin, maxmax=maxmax)\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "display_fbank(im_masked[0][start:end].squeeze(),minmin=minmin, maxmax=maxmax)\n",
    "#plt.show()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "display_fbank((y_unpatch[0][start:end]).squeeze(), minmin=minmin, maxmax=maxmax)\n",
    "#plt.show()\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(mask.shape)\n",
    "im_paste = x * (1 - mask) + y * mask\n",
    "display_fbank(im_paste[0][start:end].squeeze(),minmin=minmin, maxmax=maxmax)\n",
    "\n",
    "if model.mask_2d:\n",
    "    fn=os.path.basename(wav_file).replace('.wav',f'_2d_{model.mask_t_prob}_{model.mask_f_prob}.pdf')\n",
    "else:\n",
    "    fn=os.path.basename(wav_file).replace('.wav',f'_{mask_ratio}.pdf')\n",
    "fn=os.path.join('.',fn)\n",
    "plt.savefig(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52071eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c72e77ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([650, 128, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][start:end].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501017b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_file = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bf770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d35b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file1 = '/large_experiments/cmd/audioset/balance_wav/zye7IPXojSc.wav'\n",
    "wav_file2='/large_experiments/cmd/audioset/balance_wav/zyqg4pYEioQ.wav'\n",
    "wav_file3='/large_experiments/cmd/audioset/eval_wav/1W2FOzSXsxs.wav'\n",
    "wav_file4='/large_experiments/cmd/audioset/eval_wav/1SLrRllxMkU.wav'\n",
    "wav_file5='/large_experiments/cmd/audioset/eval_wav/1FpNkptebK8.wav'\n",
    "wav_file6='/large_experiments/cmd/audioset/eval_wav/1IrYZhVhN1s.wav'\n",
    "wav_file7='/large_experiments/cmd/audioset/eval_wav/0q1wOYCfLlQ.wav'\n",
    "wav_file8='/large_experiments/cmd/audioset/eval_wav/0qDs_aC0LwI.wav'\n",
    "wav_file9='/large_experiments/cmd/audioset/eval_wav/0qSK2GuljEc.wav'\n",
    "wav_file0='/large_experiments/cmd/audioset/eval_wav/0qWRXZkmXF8.wav'\n",
    "wav_file10='/large_experiments/cmd/audioset/eval_wav/MdYXznF3Eac.wav'\n",
    "wav_file11='/large_experiments/cmd/audioset/eval_wav/Rr84-EZvO0U.wav'\n",
    "wav_file12='/large_experiments/cmd/audioset/eval_wav/XHQGUbMSPTM.wav'\n",
    "wav_file13='/large_experiments/cmd/audioset/eval_wav/bq6C0_tAbJM.wav'\n",
    "wav_file14='/large_experiments/cmd/audioset/eval_wav/hRbukCd6N68.wav'\n",
    "wav_file15='/large_experiments/cmd/audioset/eval_wav/HV1J_actdHE.wav'\n",
    "wav_file16='/large_experiments/cmd/audioset/eval_wav/8UMdVUartLw.wav'\n",
    "wav_file17='/large_experiments/cmd/audioset/eval_wav/3Mo-YFd31rs.wav'\n",
    "wav_file18='/large_experiments/cmd/audioset/eval_wav/nT_R3O0OK6U.wav'\n",
    "wav_file19='/large_experiments/cmd/audioset/eval_wav/bvapjUmC7bY.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file0=r\"D:\\AudioMAE\\Beijing Police car siren-[AudioTrimmer.com].wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af056d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_9084\\1500003158.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(fbank)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n",
      "torch.Size([1, 513, 768])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (128) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mmask_t_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mmask_f_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mrun_one_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_file0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m, in \u001b[0;36mrun_one_audio\u001b[1;34m(wav_file, model)\u001b[0m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnchw->nhwc\u001b[39m\u001b[38;5;124m'\u001b[39m, x)\n\u001b[0;32m     23\u001b[0m im_masked \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask)\n\u001b[1;32m---> 24\u001b[0m im_paste \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m+\u001b[39m \u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[0;32m     26\u001b[0m minmin \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmin([x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(), im_masked[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(), y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(), im_paste[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()])\n\u001b[0;32m     27\u001b[0m maxmax \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax([x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(), im_masked[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(), y[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(), im_paste[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (128) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(31)\n",
    "model.mask_2d=True\n",
    "model.mask_t_prob=0.1\n",
    "model.mask_f_prob=0.1\n",
    "run_one_audio(wav_file0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4a41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0282dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
